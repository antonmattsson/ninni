
*** How to import datasets into Ninni's database *** 


1: RESULT FILES

All the datasets you wish to import to Ninni's database should be in .csv files with the following columns:

VARIABLE1_LABEL: The label ofthe first variable
VARIABLE2_LABEL: The label of the second variable (exclude this column if the dataset has only one variable)
EFFECT
EFFECT_L95
EFFECT_U95
N
P
P_FDR

Plus any additional columns

Example: study1.csv

VARIABLE1_LABEL,VARIABLE2_LABEL,EFFECT,EFFECT_L95,EFFECT_U95,N,P,P_FDR,CLASS
ABCD,TP53,8,5,10,100,0.01,1,A

NOTE: Any additional columns will be imported as numeric values if they look like numeric values


2: VARIABLE FILES

You can add a .csv file describing the variables.
If this file is not specified for the dataset, variable description is set to be the same as the variable label. 
The file should have the following columns:

LABEL: The variable label, same as in the previous csv file
DESCRIPTION: Description of the variable

Example: study1_variables.csv:

LABEL,DESCRIPTION
ABCD,Best known gene to man-kind
TP53,Cancer gene




3: LIST DATASETS

The datasets should be listed in a .csv file with the following columns:

- DATASET_FILENAME: the name of the .csv file containing the dataset
- VARIABLES_FILENAME: optional file describing the variables
- LABEL: short label for the dataset
- DESCRIPTION: longer description of the dataset
- VARNUM: number of variables per association in the dataset (1 or 2)
- EFFECT_TYPE: The effect type in the dataset: "OR" for odds-ratio, "FC" for fold-change or "CORR" for correlation
- METADATA_LABELS: possible metadata labels for the dataset, separated by ';'

NOTE: it is recommended to use relative path from src folder as filenames, since the import script will most likely be run from the src folder

Example: datasets.csv:

DATASET_FILENAME,VARIABLES_FILENAME	LABEL,DESCRIPTION,VARNUM,EFFECT_TYPE,METADATA_LABELS
../data/study1_results.csv,../data/study1_variables.csv,DRUG_STUDY1,Drug interaction study using mortality as outcome,2,OR,DRUG_INTERACTION;MALES;T2D



4: DATASET METADATA

Metadata labels of datasets should be described in a separate .csv file with the following columns:

LABEL: The metadata label, same as in the previous .csv file listing datasets
DESCRIPTION: Description of the metadata label


Example: metadata.csv

LABEL,DESCRIPTION
DRUG_INTERACTION,Drug interaction study
MALE,Only males
T2D,Type 2 Diabetes

5: IMPORTING DATA

When all the files are in the right format, you can import the datasets using import_data.py script found in the /src directory.

The script has 4 command line parameters:

-dsf, --dataset_file 	dataset file (see section 3)
-mdf, --meta_data_file 	metadata file (see section 4)
-a --append				Add dataset to the databse instead of clearing the database before import
-ml --maxlines			Maximum number of lines imported per dataset (mainly for testing, defaults to unlimited)

!! NOTE !! By default, the database schema is dropped i.e. all the data in the database is deleted prior to each import.
If you wish to append datasets to the database without deleting existing data, you can use -a or --append flag.
-a will append any NEW datasets to the database and fail if there already exists a dataset with the same label

Example data is the example_data folder:

- The datasets are stored as .csv files
- The .csv files are listed in datasets.csv file
- The example_data folder also contains the metadata.csv file

- Now, running the following command in the src imports the datasets into Ninni's database:

$ python import_data.py -dsf ../example_data/datasets.csv -mdf ../example_data/metadata.csv

NOTE: you need to have the proper version of python and psycopg2 library installes, see SETUP.txt for more information.

If you are using conda and have the conda environment installed, you can use the import_example_data.sh bash script located in the src folder for facilitated import.
The import.sh script activates the conda environment, imports the data and then deactivates the conda environment, making sure that everything runs smoothly.
All you need to do is to make sure that the dataset and metadata file names are correct.

